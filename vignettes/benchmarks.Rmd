---
title: "Benchmarks"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Benchmarks}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

Measure the performance of different implementations of `cards` using `bench::mark()`.

```{r setup, message = FALSE, warning = FALSE}
library(cards)

deck <- new_deck_df()
```

# Deal

Test performance of  compared to selecting 5 numbers from 1:52:

```{r deal_hand_df}
bench::mark(deal_hand_df(deck))
bench::mark(sample(1:52, 5))
```

`deal_hand_df()` is 7x slower than raw `sample()`.

# Print

Test performance of `print_hand()` with a single hand and with randomly selected hands:

```{r print_hand}
test_hand <- deal_hand_df(deck)
bench::mark(print_hand(test_hand))
bench::mark(print_hand(deal_hand_df(deck)))
```

Performance of `deal_hand_df()` is slower than `eval_hand()`.

# Evaluate

Test performance of `eval_hand()` with a single hand and with randomly selected hands:

```{r eval_hand}
test_hand <- deal_hand_df(deck)
bench::mark(eval_hand(test_hand))
bench::mark(eval_hand(deal_hand_df(deck)))
```

As expected for a naive poker hand evaluator, performance of `eval_hand()` is poor compared to fast algorithms.
